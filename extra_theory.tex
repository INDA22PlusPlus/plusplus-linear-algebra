\documentclass{report}

\include{preamble}
\include{macros}
\include{letterfonts}

\usepackage[Glenn]{fncychap}

\title{\Huge{SF1624 Linjär algebra och geometri, bra formler}}
\author{\huge{}}
\date{}


\begin{document}

\maketitle
\newpage
\pagebreak

\chapter{Satser, definitioner}

\nt{\textbf{Synonymer för bildrummet av matrisen $A$}: $\:\:\:Im(A),\:Range(A),\:Col(A)$}
\nt{\textbf{Synonymer för nollrummet av matrisen $A$}: $\:\:\:ker(A),\:null(A)$}

\thm{Matrisen som projekterar på $ \vec{v}  $ }
{
	För att slippa hålla på med beräkningar med bråktal för att hitta matrisen som beskriver projektionen på vektorn $ \vec{v}  $, så kan man använda formeln nedan:
\[
A = \frac{1}{\vec{v} \cdot \vec{v} } \vec{v} \vec{v}^T
\]
Därmed projektionen av vektorn $ \vec{x} $ på vektorn $ \vec{v}  $ beskrivs av matrismultiplikationen $ proj_{ \vec{v} } \vec{x} = A \vec{x}  $  
}

\thm{Matrisen som projekterar på vektorrummet $ V $ }
{
Om vektorummet definieras som $ V := col(A) $, då beskrivs matrisen som projekterar på vektorrummet $ V $ på följande sättet:
\[
	P = A (A^T A)^{-1} A^T
\]
Alltså för att projektera givna vektorn $ \vec{x}  $ på vektorummet $ V $, så använder man följande matrismultiplikation $ P \vec{x}  $. \textbf{Notera} att det är exakt samma metod som används för minstakvadratmetoden. Projektionen av en vektor på en vektorummet ger den bästa approximationen av givna vektorn på vektorrummet. 
}

\pagebreak

\thm{Ortogonala komplementet till delrummet $ V \in \mathbb{R}^n $ }
{
Om man vill hitta ortogonala komplementet (också delrum) till delrummet $ V $ med villkorn att $ V $ inte spannar hela $ \mathbb{R}^n $, så använder man formeln nedan. \textbf{Observera} att $ V := col(A) $
\[
	V^\perp = ker(A^T) = null(A^T)
\]
	\textbf{Varför}: En ortogonal komplement $ V^\perp $ till delrummet $ V $ innebär att $ \forall \vec{v} \in V^\perp,\: \forall \vec{u} \in V \implies  v \cdot u = 0$. Om $ A $ beskrivs som $ 
\begin{bmatrix}
	w_1 & \ldots  & w_k \\
\end{bmatrix}
$ så kommer $ A^T $ beskrivas på sättet nedan.

\[ A^T =
\begin{bmatrix}
	w_1 \\
	\vdots \\
	w_k \\
\end{bmatrix}
\]
Om man multiplicerar $ A^T $ med en vektor $ \vec{x}  $ och försöker bestämma noll-rummet så bestämmer vi per definition ortogonala komponentet. D.v.s rummet där varje vektor $ \vec{x} \in \mathbb{R}^n $ ger 0 med skalärprodukten av varje vektor som spannar V ($ w_1, \ldots w_k $), som det kan ses nedan.

\[
null(A^T) = ker(A^T) :=
\begin{bmatrix}
	w_1 \cdot \vec{x}  \\
	\vdots \\
	w_k \cdot \vec{x} \\
\end{bmatrix} =
\begin{bmatrix}
	0 \\
	\vdots \\
	0 \\
\end{bmatrix}
\]

}

\thm{En vektor $ \vec{x}  $ kan skrivas som projektionen på en vektorrum + projektionens ortogonala komplement}
{
För att kunna bevisa 1.0.2 så brukar man använda denna sats som beskrivs nedan.
\[
	\vec{x} = proj_W \vec{x} + proj_{W^\perp} \vec{x}  
\]
\[
		proj_{W^\perp} \vec{x} = \vec{x} - proj_W \vec{x} 
\]


}

\thm{Hitta resterande basvektorer i $ \mathbb{R}^n $ utifrån en mängd linjärt oberoende vektorer $ S $}
{
För det, måste storleken av $ S $ vara mindre än $ n $, annars är $ S $ redan en bas för $ \mathbb{R}^n $. Om $ S = \{ w_1, \cdots, w_k\} $, så sätter vi upp dessa vektorer som kolumnelement och sedan löser noll-rummet, som det kan ses nedan.
\[
A =
\begin{bmatrix}
	w_1 \\
	\vdots \\
	w_k
\end{bmatrix}; \: B := null(A) = ker(A)
\]
\textbf{Varför}: Som i 1.0.3 så försöker vi hitta en mängd vektorer ($ B $) som är ortogonala och därmed linjärt oberoende mot varje vektor i $ S $. Detta är ekvivalent med att varje vektor i mängden vektorer vi försöker lösa, $ B $, har skalärprodukten 0 med varje vektor i $ S $. Enligt kraven för linjärt oberoendet av basvektorerna så blir mängden $ S \cup B $ en bas för $ \mathbb{R}^n $.  
}

\pagebreak

\thm{Sambandet mellan matrisen $A$ och $A^T A$, samt $A A^T$}
{
Theoreum 7.5.8 \& 7.5.9 i boken Contemporary Linear Algebra (s. 365)
\begin{itemize}
	\item $A$ och $A A^T$ har samma kolumnrum
	\item $A$ och $A^T A$ har samma radrum
	\item Om $A$ har full kolumnrank $ \implies det(A^T A) \ne 0 $
	\item Om $A$ har full radrank $ \implies det(A A^T) \ne 0 $
\end{itemize}
\textbf{Vad} kan man använda detta till? Om man vill kolla för en större matris om raderna eller kolumnerna är linjärt oberoende, så kan man bestämma determinanten av $A^T A$ respektive $A A^T$. Om determinanten $ \ne 0$ då medför det att kolumerna respektive raderna i matrisen $A$ är linjärt oberoende. \textbf{OBS}: $A A^T$ och $A^T A$ är kvadratiska matriser.  
}

\thm{Ortogonal diagonalisering}
{
Symmetriska matriser ($ A^{-1} = A^T $) är ortogonalt diagonaliserbara och kan därmed utryckas som $ A = P D P^T $. Dessutom när det kommer till egenvärde och diagonalisering har symmetriska matriser följande egenskaper (A är en $n \times n$ symmetrisk matris):
\begin{itemize}
	\item $ A $ har $ n $ olika reella egenvärden, räknade med multiplicitet.
	\item Dimensionen av varje egenrum överensstämmer med tillhörande egenvärdes multiplicitet som rot till karaktäristiska ekvationen.
	\item Egenvektorerna från de olika egenvärden är ortogonala mot varandra $ \implies $ spannar upp hela $ \mathbb{R}^n $.
\end{itemize}
}

\thm{Symmetriska matriser för kvadratiska former}
{
Om $A$ är en symmetrisk matris för den kvadratiska formen $x^T A x$ så gäller följande satser:
\begin{itemize}
	\item $x^T A x$ är positivt definit ($x^T A x > 0, \forall x \ne \vec{0}$) om och endast om \textbf{alla} egenvärden av $A$ är positiva
	\item $x^T A x$ är negativt definit ($x^T A x < 0, \forall x \ne \vec{0}$) om och endast om \textbf{alla} egenvärden av $A$ är negativa
	\item $x^T A x$ är indefinit ($x^T A x > 0 \land x^T A x < 0, \forall x$) om och endast om $A$ har minst en positiv och en negativ egenvärde
\end{itemize}
}

\thm{Cayley-Hamilton sats}
{
S. 474 i boken "Contemporary Linear Algebra"\\\\

En kvadratisk matris $A$ med storleken $n \times n$ uppfyller sin motsvarande karaktäristiska ekvation, det vill säga att om karaktäristiska ekvationen för matrisen $A$ är:
\[
\lambda^n + c_1 \lambda^{n-1} + \cdots + c_n = 0
\]
Så gäller följande:
\[
A^n + c_1 A^{n-1} + \cdots + c_n I = 0
\]
}

\pagebreak

\thm{Multiplikation mellan en matris och en vektor}
{
S. 106 i boken "Contemporary Linear Algebra"\\
Följande två viktiga satser gäller för multiplikationen av vektorerna $ \vec{u}, \vec{v} $ med matrisen $A$.
\begin{itemize}
	\item $A \vec{u} \cdot \vec{v} = \vec{u} \cdot A^T \vec{v}$ 
	\item $\vec{u} \cdot A \vec{v} = A^T \vec{u} \cdot \vec{v}$
\end{itemize}
}

\thm{Fundamentala sambandet mellan rummet $col(A)$ och dess ortogonala komplement}
{
S. 344---345 i boken "Contemporary Linear Algebra".\\\\

Följande samband gäller som kan vara praktiska under tentan.
\[
row(A)^\perp = null(A), \:\:\: null(A)^\perp = row(A)
\]
\[
col(A)^\perp = null(A^T), \:\:\: null(A^T)^\perp = col(A)
\]
}

\dfn{Extra om isomorfism}
{
	Isomorfism = bijektiv.\\
	Läs mer om isomorfism i linjär algebra, \href{https://math.libretexts.org/Bookshelves/Linear_Algebra/A_First_Course_in_Linear_Algebra_(Kuttler)/05\%3A_Linear_Transformations/5.06\%3A_Isomorphisms}{här}.
	\\\\
	
	För att bevisa att transformationen $T: V \mapsto W$ med matrisen $A$ är isomorfisk så måste man bevisa följande 3 punkter:
	\begin{itemize}
		\item $T(\vec{x})$ är en linjär transformation ($\:T(k\vec{x}) = kT(\vec{x}); \:\:\: T(\vec{a} + \vec{b}) = T(\vec{a}) + T(\vec{b})\:$)
		\item $T(\vec{x}) = T(\vec{y}) \implies \vec{x} = \vec{y}$ (\textbf{Injektiv}, $\vec{x}$ är unik)
		\item $\vec{w} \in W \implies \exists \: \vec{v} \in V$ så att $T(\vec{v}) = \vec{w}$
	\end{itemize}
	Sista punkten kallas för att vara \textit{Onto} på engelska. Att \textbf{inte} vara \textit{Onto} kan beskrivas på dessa tre ekvivalenta sätt:
	\begin{itemize}
		\item $dim(V) < dim(W)$
		\item Det existerar en vektor $\vec{b} \in W$ så att $T(\vec{x}) = \vec{b}$ inte har en lösning.
		\item Det existerar en vektor i $W$ som inte är en output av transformationen $T$. 
	\end{itemize}
	För att bevisa att transformationen $T$ med matrisen $A$ följer punkt 2, måste man visa att $ker(A)$ består endast av $\vec{0}$, \textbf{alltså} att $ker(A)$ \textbf{inte} har icke-triviala lösningar.\\\\
	
	För att bevisa att transformationen $T$ med matrisen $A$ följer punkt 3, måste man visa att $A\vec{x} = \vec{b}$ uppfylls för varje möjlig $\vec{b}$. Detta görs genom att Gauss-eliminiera matrisen $
\begin{bmatrix}
	A & | & \vec{b}
\end{bmatrix}	
$ och undersöka att högersidan kan alltid uppfyllas. 
}

\end{document}
